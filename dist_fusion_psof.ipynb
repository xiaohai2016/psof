{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage of PSOF in two layer distributed fusion\n",
    "\n",
    "In S.-L. Sun and Z.-L. Deng, “Multi-sensor optimal information fusion kalman filter,” Automatica, vol. 40, no. 6, pp. 1017–1023, Jun. 2004. [Online]. Available: http://dx.doi.org/10.1016/j.automatica.2004.01.014, a two-layer fusion structure is proposed. The proposed structure requires computation of cross-correlations among distributed node, and hence require time sync and exchange of Kalman gain matrices. We propose to replace the second layer with PSOF-based data fusion algorithm to eliminate the need for time sync and further information exchange other than estimates and their covariances. In addition to the saved communication cost, the PSOF-based approach enhances system reliability and design flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparative simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Methods based on Sun's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_kalman(x_t_t, p_t_t, y_tplus1, y_t, \\\n",
    "                 phi_t, gamma_t, h_tplus1, h_t, \\\n",
    "                 q_t, r_tplus1, r_t, s_t, b_t, u_t, \\\n",
    "                 w_t=np.zeros((1,1))):\n",
    "    \"\"\"\n",
    "    Given previous estimates & current measurements\n",
    "    to return updated estimates and Kalman gain\n",
    "    Based on Kalman filtering formulation given in Sun's paper\n",
    "    \"\"\"\n",
    "    i_n = np.identity(phi_t.shape[1])\n",
    "    r_t_inv = np.linalg.inv(r_t)\n",
    "    j_t = gamma_t @ s_t @ r_t_inv\n",
    "    phi_t_bar = phi_t - j_t @ h_t\n",
    "    x_tplus1_t = phi_t_bar @ x_t_t + b_t @ u_t + j_t @ y_t + gamma_t @ w_t\n",
    "    epsilon_tplus1 = y_tplus1 - h_tplus1 @ x_tplus1_t\n",
    "    p_tplus1_t = phi_t_bar @ p_t_t @ phi_t_bar.T + \\\n",
    "                 gamma_t @ (q_t - s_t @ r_t_inv @ s_t.T) @ gamma_t.T\n",
    "    k_tplus1 = p_tplus1_t @ h_tplus1.T @ np.linalg.inv(h_tplus1 @ p_tplus1_t @ h_tplus1.T + r_tplus1)\n",
    "    x_tplus1_tplus1 = x_tplus1_t + k_tplus1 @ epsilon_tplus1\n",
    "    p_tplus1_tplus1 = (i_n - k_tplus1 @ h_tplus1) @ p_tplus1_t\n",
    "    return x_tplus1_tplus1, p_tplus1_tplus1, k_tplus1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cross_covariance(phi_t, \\\n",
    "                         k_i_tplus1, k_i_t, k_j_tplus1, k_j_t, \\\n",
    "                         h_i_tplus1, h_i_t, h_j_tplus1, h_j_t, \\\n",
    "                         r_i_t, r_j_t, s_i_t, s_j_t, \\\n",
    "                         gamma_t, q_t, p_ij_t_t, s_ij_t, s_ij_tplus1):\n",
    "    \"\"\"\n",
    "    Based on theorem 2 of Sun's paper\n",
    "    \"\"\"\n",
    "    i_n = np.identity(phi_t.shape[1])\n",
    "\n",
    "    r_i_t_inv = np.linalg.inv(r_i_t)\n",
    "    j_i_t = gamma_t @ s_i_t @ r_i_t_inv\n",
    "    phi_i_t_bar = phi_t - j_i_t @ h_i_t\n",
    "    \n",
    "    r_j_t_inv = np.linalg.inv(r_j_t)\n",
    "    j_j_t = gamma_t @ s_j_t @ r_j_t_inv\n",
    "    phi_j_t_bar = phi_t - j_j_t @ h_j_t\n",
    "    \n",
    "    return (i_n - k_i_tplus1 @ h_i_tplus1) @ \\\n",
    "           (phi_i_t_bar @ p_ij_t_t @ phi_j_t_bar.T + \\\n",
    "            gamma_t @ q_t @ gamma_t.T - j_j_t @ r_j_t @ j_j_t.T - \\\n",
    "            j_i_t @ r_i_t @ j_i_t.T + j_i_t @ s_ij_t @ j_j_t.T + \\\n",
    "            phi_i_t_bar @ k_i_t @ (s_ij_t @ j_j_t.T - s_i_t.T @ gamma_t.T) + \\\n",
    "            (j_i_t @ s_ij_t - gamma_t @ s_j_t) @ k_j_t.T @ phi_j_t_bar.T) @ \\\n",
    "           (i_n - k_j_tplus1 @ h_j_tplus1).T + \\\n",
    "           k_i_tplus1 @ s_ij_tplus1 @ k_j_tplus1.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_fusion_known_cross_covariances(x_is, p_ijs):\n",
    "    \"\"\"\n",
    "    Optimal fusion with known cross-covariances.\n",
    "    Based on theorem 1 of Sun's paper\n",
    "    \"\"\"\n",
    "    l = len(x_is)\n",
    "    assert(len(p_ijs) == l * l)\n",
    "    n = x_is[0].shape[0]\n",
    "    assert(p_ijs[0].shape == (n, n))\n",
    "    e = np.concatenate([np.identity(n) for _ in range(l)])\n",
    "    Sigma = np.concatenate([np.concatenate(p_ijs[i*l:(i+1)*l], axis=1) for i in range(l)])\n",
    "    # The Sigma matrix was observed to be often ill-conditioned,\n",
    "    # hence we might need to add 1e-10 * I to correct\n",
    "    Sigma_inv = np.linalg.inv(Sigma) # + 1e-10 * np.identity(n*l))\n",
    "    P_0 = np.linalg.inv(e.T @ Sigma_inv @ e)\n",
    "    A_bar = Sigma_inv @ e @ P_0\n",
    "    x_0 = A_bar.T @ np.concatenate(x_is)\n",
    "    return x_0, P_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter initializations based on Sun's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 0.01\n",
    "gamma_tplus1 = gamma_t = np.array([[0.0, 0.0, 1.0]], dtype=float).T\n",
    "phi_tplus1 = phi_t = np.array([[1.0, T, T * T / 2.0], [0.0, 1.0, T], [0.0, 0.0, 1.0]], dtype=float)\n",
    "h_1_tplus1 = h_1_t = np.array([[1.0, 0.0, 0.0]], dtype=float)\n",
    "h_2_tplus1 = h_2_t = np.array([[0.0, 1.0, 0.0]], dtype=float)\n",
    "h_3_tplus1 = h_3_t = np.array([[0.0, 0.0, 1.0]], dtype=float)\n",
    "h_tplus1 = h_t = np.concatenate((h_1_tplus1, h_2_tplus1, h_3_tplus1))\n",
    "sigma_w_square = 1.0\n",
    "sigma_zeta_1_square = 5.0\n",
    "sigma_zeta_2_square = 8.0\n",
    "sigma_zeta_3_square = 10.0\n",
    "alpha_1 = 0.5\n",
    "alpha_2 = 0.8\n",
    "alpha_3 = 0.4\n",
    "x_0 = np.full((3, 1), 0.0, dtype=float)\n",
    "p_0 = 0.1 * np.identity(3)\n",
    "s_1_tplus1 = s_1_t = np.full((1,1), alpha_1 * sigma_w_square, dtype=float)\n",
    "s_2_tplus1 = s_2_t = np.full((1,1), alpha_2 * sigma_w_square, dtype=float)\n",
    "s_3_tplus1 = s_3_t = np.full((1,1), alpha_3 * sigma_w_square, dtype=float)\n",
    "s_21_tplus1 = s_21_t = s_12_tplus1 = s_12_t = np.full((1,1), alpha_1 * alpha_2 * sigma_w_square, dtype=float)\n",
    "s_31_tplus1 = s_31_t = s_13_tplus1 = s_13_t = np.full((1,1), alpha_1 * alpha_3 * sigma_w_square, dtype=float)\n",
    "s_32_tplus1 = s_32_t = s_23_tplus1 = s_23_t = np.full((1,1), alpha_2 * alpha_3 * sigma_w_square, dtype=float)\n",
    "s_tplus1 = s_t = np.concatenate((s_1_t, s_2_t, s_3_t)).T\n",
    "\n",
    "q_t = np.full((1,1), sigma_w_square, dtype=float)\n",
    "r_1_tplus1 = r_1_t = np.full((1,1), sigma_zeta_1_square + alpha_1 * alpha_1 * sigma_w_square, dtype=float)\n",
    "r_2_tplus1 = r_2_t = np.full((1,1), sigma_zeta_2_square + alpha_2 * alpha_2 * sigma_w_square, dtype=float)\n",
    "r_3_tplus1 = r_3_t = np.full((1,1), sigma_zeta_3_square + alpha_3 * alpha_3 * sigma_w_square, dtype=float)\n",
    "r_tplus1 = r_t = np.array([[r_1_t[0, 0], s_12_t[0, 0], s_13_t[0, 0]], \\\n",
    "                           [s_12_t[0, 0], r_2_t[0, 0], s_23_t[0, 0]], \\\n",
    "                           [s_13_t[0, 0], s_23_t[0, 0], r_3_t[0, 0]]], dtype=float)\n",
    "\n",
    "y_tplus1 = y_t = np.zeros((3,1))\n",
    "y_1_tplus1 = y_1_t = np.zeros((1,1))\n",
    "y_2_tplus1 = y_2_t = np.zeros((1,1))\n",
    "y_3_tplus1 = y_3_t = np.zeros((1,1))\n",
    "\n",
    "b_t = b_3_t = b_2_t = b_1_t = np.zeros((3,1))\n",
    "u_t = u_3_t = u_2_t = u_1_t = np.zeros((1,1))\n",
    "\n",
    "iteration_count = 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reproducing some of the results in Sun's paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Local Kalman filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_kf_variances = []\n",
    "x_3 = x_2 = x_1 = x_0\n",
    "p_3 = p_2 = p_1 = p_0\n",
    "local_kf_variances.append([p_1, p_2, p_3])\n",
    "for _ in range(iteration_count):\n",
    "    x_1, p_1, _ = local_kalman(x_1, p_1, y_1_tplus1, y_1_t, \\\n",
    "                            phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                            q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t)\n",
    "    x_2, p_2, _ = local_kalman(x_2, p_2, y_2_tplus1, y_2_t, \\\n",
    "                            phi_t, gamma_t, h_2_tplus1, h_2_t, \\\n",
    "                            q_t, r_2_tplus1, r_2_t, s_2_t, b_2_t, u_2_t)\n",
    "    x_3, p_3, _ = local_kalman(x_3, p_3, y_3_tplus1, y_3_t, \\\n",
    "                            phi_t, gamma_t, h_3_tplus1, h_3_t, \\\n",
    "                            q_t, r_3_tplus1, r_3_t, s_3_t, b_3_t, u_3_t)\n",
    "    local_kf_variances.append([p_1, p_2, p_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Centralized Kalman filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centralized_kf_variances = []\n",
    "x_c = x_0\n",
    "p_c = p_0\n",
    "centralized_kf_variances.append(p_c)\n",
    "for _ in range(iteration_count):\n",
    "    x_c, p_c, _ = local_kalman(x_c, p_c, y_tplus1, y_t, \\\n",
    "                            phi_t, gamma_t, h_tplus1, h_t, \\\n",
    "                            q_t, r_tplus1, r_t, s_t, b_t, u_t)\n",
    "    centralized_kf_variances.append(p_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Two-layer filtering in Sun's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "two_layers_kf_variances = []\n",
    "x_3 = x_2 = x_1 = x_0\n",
    "p_21 = p_31 = p_32 = p_12 = p_13 = p_23 = p_0\n",
    "p_3 = p_2 = p_1 = p_0\n",
    "k_1_t = k_1_tplus1 = np.zeros((3,1))\n",
    "k_2_t = k_2_tplus1 = np.zeros((3,1))\n",
    "k_3_t = k_3_tplus1 = np.zeros((3,1))\n",
    "two_layers_kf_variances.append(p_1)\n",
    "for _ in range(iteration_count):\n",
    "    k_1_t = k_1_tplus1\n",
    "    k_2_t = k_2_tplus1\n",
    "    k_3_t = k_3_tplus1\n",
    "    x_1, p_1, k_1_tplus1 = local_kalman(x_1, p_1, y_1_tplus1, y_1_t, \\\n",
    "                            phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                            q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t)\n",
    "    x_2, p_2, k_2_tplus1 = local_kalman(x_2, p_2, y_2_tplus1, y_2_t, \\\n",
    "                            phi_t, gamma_t, h_2_tplus1, h_2_t, \\\n",
    "                            q_t, r_2_tplus1, r_2_t, s_2_t, b_2_t, u_2_t)\n",
    "    x_3, p_3, k_3_tplus1 = local_kalman(x_3, p_3, y_3_tplus1, y_3_t, \\\n",
    "                            phi_t, gamma_t, h_3_tplus1, h_3_t, \\\n",
    "                            q_t, r_3_tplus1, r_3_t, s_3_t, b_3_t, u_3_t)\n",
    "    p_12 = compute_cross_covariance(phi_t, \\\n",
    "                         k_1_tplus1, k_1_t, k_2_tplus1, k_2_t, \\\n",
    "                         h_1_tplus1, h_1_t, h_2_tplus1, h_2_t, \\\n",
    "                         r_1_t, r_2_t, s_1_t, s_2_t, \\\n",
    "                         gamma_t, q_t, p_12, s_12_t, s_12_tplus1)\n",
    "    p_13 = compute_cross_covariance(phi_t, \\\n",
    "                         k_1_tplus1, k_1_t, k_3_tplus1, k_3_t, \\\n",
    "                         h_1_tplus1, h_1_t, h_3_tplus1, h_3_t, \\\n",
    "                         r_1_t, r_3_t, s_1_t, s_3_t, \\\n",
    "                         gamma_t, q_t, p_13, s_13_t, s_13_tplus1)\n",
    "    p_23 = compute_cross_covariance(phi_t, \\\n",
    "                         k_2_tplus1, k_2_t, k_3_tplus1, k_3_t, \\\n",
    "                         h_2_tplus1, h_2_t, h_3_tplus1, h_3_t, \\\n",
    "                         r_2_t, r_3_t, s_2_t, s_3_t, \\\n",
    "                         gamma_t, q_t, p_23, s_23_t, s_23_tplus1)\n",
    "    p_21 = p_12.T\n",
    "    p_31 = p_13.T\n",
    "    p_32 = p_23.T\n",
    "    x_0_n, p_0_n = optimal_fusion_known_cross_covariances([x_1, x_2, x_3], \\\n",
    "                         [p_1, p_12, p_13, p_21, p_2, p_23, p_31, p_32, p_3])\n",
    "    two_layers_kf_variances.append(p_0_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributed filtering assuming independence (Monte Carlo simulation)\n",
    "1.   We first run the Monte Carlo simulation against Kalman filtering with node 1 alone to sanity check the simulation codes.\n",
    "2.   Then we duplicate the codes to run the same simulation with the crudest assumption of independence.\n",
    "3. In simulation, we assume the system state is fixed to be (0, 0, 0), i.e. no movement or acceleration. To simulate the initial state uncertainty, we use random initialization instead to achieve the goal of x_0 having covariance 0.1 I_3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Simulations for sanity check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simulation_rounds = 300\n",
    "\n",
    "w_t = np.random.normal(0.0, math.sqrt(sigma_w_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_1 = np.random.normal(0.0, math.sqrt(sigma_zeta_1_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "measure_noise_1 = alpha_1 * w_t + zeta_1\n",
    "\n",
    "x_o_1 = np.zeros((3, iteration_count+1, simulation_rounds), dtype=float)\n",
    "\n",
    "for r in range(simulation_rounds):\n",
    "    y_1_tplus1 = 0.0\n",
    "    p_1 = p_0\n",
    "    init_std = math.sqrt(p_0[0, 0])\n",
    "    x_1 = np.random.normal(0.0, init_std, (3,1))\n",
    "    x_o_1[0, 0, r] = x_1[0,0]\n",
    "    x_o_1[1, 0, r] = x_1[1,0]\n",
    "    x_o_1[2, 0, r] = x_1[2,0]\n",
    "    for i in range(iteration_count):\n",
    "        y_1_t = y_1_tplus1\n",
    "        y_1_tplus1 = measure_noise_1[i, r]\n",
    "        x_1, p_1, _ = local_kalman(x_1, p_1, np.full((1,1), y_1_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_1_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                                q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        x_o_1[0, i+1, r] = x_1[0, 0]\n",
    "        x_o_1[1, i+1, r] = x_1[1, 0]\n",
    "        x_o_1[2, i+1, r] = x_1[2, 0]\n",
    "x_o_1_vars = np.var(x_o_1, axis=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "xlims = [(0, 100), (0, 300), (0, 10)]\n",
    "ylims = [(0.0, 0.38), (0.07, 4), (1, 10)]\n",
    "labels = ['position', 'velocity', 'acceleration']\n",
    "for i in range(3):\n",
    "    axes[i].set_xlabel('time step')\n",
    "    axes[i].set_ylabel(f'{labels[i]} variance')\n",
    "    axes[i].plot(x_o_1_vars[i,:], label=\"Monte Carlo sim\")\n",
    "    axes[i].plot([p[0][i,i] for p in local_kf_variances], label='local filter 1')\n",
    "    axes[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Monte Carlo simulation for distributed filtering crudely assuming independence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independence assumptions\n",
    "p_21 = p_31 = p_32 = p_12 = p_13 = p_23 = np.zeros((3,3))\n",
    "\n",
    "simulation_rounds = 50\n",
    "\n",
    "w_t = np.random.normal(0.0, math.sqrt(sigma_w_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_1 = np.random.normal(0.0, math.sqrt(sigma_zeta_1_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_2 = np.random.normal(0.0, math.sqrt(sigma_zeta_2_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_3 = np.random.normal(0.0, math.sqrt(sigma_zeta_3_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "measure_noise_1 = alpha_1 * w_t + zeta_1\n",
    "measure_noise_2 = alpha_2 * w_t + zeta_2\n",
    "measure_noise_3 = alpha_3 * w_t + zeta_3\n",
    "\n",
    "x_o = np.zeros((3, iteration_count+1, simulation_rounds), dtype=float)\n",
    "\n",
    "for r in range(simulation_rounds):\n",
    "    y_3_tplus1 = y_2_tplus1 = y_1_tplus1 = 0.0\n",
    "    p_3 = p_2 = p_1 = p_0\n",
    "    init_std = math.sqrt(p_0[0, 0])\n",
    "    # start with correlated initialization\n",
    "    x_3 = x_2 = x_1 = np.random.normal(0.0, init_std, (3,1))\n",
    "    sampled_node = x_1\n",
    "    x_o[0, 0, r] = sampled_node[0,0]\n",
    "    x_o[1, 0, r] = sampled_node[1,0]\n",
    "    x_o[2, 0, r] = sampled_node[2,0]\n",
    "    for i in range(iteration_count):\n",
    "        y_1_t = y_1_tplus1\n",
    "        y_2_t = y_2_tplus1\n",
    "        y_3_t = y_3_tplus1\n",
    "        # measurement noises are still independent\n",
    "        y_1_tplus1 = measure_noise_1[i, r]\n",
    "        y_2_tplus1 = measure_noise_2[i, r]\n",
    "        y_3_tplus1 = measure_noise_3[i, r]\n",
    "        x_1, p_1, _ = local_kalman(x_1, p_1, np.full((1,1), y_1_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_1_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                                q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        x_2, p_2, _ = local_kalman(x_2, p_2, np.full((1,1), y_2_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_2_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_2_tplus1, h_2_t, \\\n",
    "                                q_t, r_2_tplus1, r_2_t, s_2_t, b_2_t, u_2_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        x_3, p_3, _ = local_kalman(x_3, p_3, np.full((1,1), y_3_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_3_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_3_tplus1, h_3_t, \\\n",
    "                                q_t, r_3_tplus1, r_3_t, s_3_t, b_3_t, u_3_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        \n",
    "        # Data fusion assuming independence\n",
    "        x_0_n, p_0_n = optimal_fusion_known_cross_covariances([x_1, x_2, x_3], \\\n",
    "                             [p_1, p_12, p_13, p_21, p_2, p_23, p_31, p_32, p_3])\n",
    "        x_3 = x_2 = x_1 = x_0_n\n",
    "        p_3 = p_2 = p_1 = p_0_n\n",
    "        \n",
    "        # Track (and plot) data fusion in the first node\n",
    "        sampled_node = x_1\n",
    "        x_o[0, i+1, r] = sampled_node[:1]\n",
    "        x_o[1, i+1, r] = sampled_node[1:2]\n",
    "        x_o[2, i+1, r] = sampled_node[2:]\n",
    "x_o_vars = np.var(x_o, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PSOF codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def func_constant_step_size(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    return 0.0004\n",
    "\n",
    "def func_constant_step_length(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    global csl_initial_norm\n",
    "    N,M,_ = sub_gradient.shape\n",
    "    norm = 0\n",
    "    for i in range(N):\n",
    "        norm += np.trace(np.matmul(sub_gradient[i], np.transpose(sub_gradient[i])))\n",
    "    return 0.002 / np.sqrt(norm)\n",
    "        \n",
    "def func_square_summable_not_summable(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    return 3.0 / (iteration + 1)\n",
    "\n",
    "def func_not_summable_diminishing_step_size(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    return 0.035 / np.sqrt(iteration + 1)\n",
    "\n",
    "def func_not_summable_diminishing_step_length(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    global nsdsl_initial_norm\n",
    "    N,M,_ = sub_gradient.shape\n",
    "    norm = 0\n",
    "    for i in range(N):\n",
    "        norm += np.trace(np.matmul(sub_gradient[i], np.transpose(sub_gradient[i])))\n",
    "    return 0.19 / np.sqrt(norm * (iteration + 1))\n",
    "\n",
    "def func_polyak_with_estimate(sub_gradient, iteration, diff_from_best_estimate):\n",
    "    global polyak_initial_norm\n",
    "    N,M,_ = sub_gradient.shape\n",
    "    norm = 0\n",
    "    for i in range(N):\n",
    "        norm += np.trace(np.matmul(sub_gradient[i], np.transpose(sub_gradient[i])))\n",
    "    return (0.035 * norm / np.sqrt(iteration + 1) + diff_from_best_estimate) / norm\n",
    "\n",
    "# Projected Subgradient Method\n",
    "\n",
    "# Usage:\n",
    "#   a_i, mse, v_opt, v_ij_opt = psof (\n",
    "#    N, M, \n",
    "#    joint_covariance, \n",
    "#    unknown_index_matrix)\n",
    "#\n",
    "# Inputs:\n",
    "#   N - the input measurement count\n",
    "#   M - the input measurement dimension\n",
    "#   joint_covariance - the joint covariance matrix\n",
    "#       for the all the measurements. It's a 4-d\n",
    "#       tensor, with the first two dimensions\n",
    "#       referring to the measurements and the last\n",
    "#       two dimensions referring to the measurement\n",
    "#       components. For unknown cross-correlation\n",
    "#       matrices, the values are not used.\n",
    "#   unknown_index_matrix - a bool numpy array.\n",
    "#       the element of the matrix at\n",
    "#       location (i,j) is set to be one if V_ij is unknown.\n",
    "#       otherwise it is set to be zero.\n",
    "#\n",
    "# Outputs:\n",
    "# . a_i - the matrix weights, a tensor of 3d, with the first\n",
    "# .       dimension index being the measurement index\n",
    "# . mse - the resulting mean square error\n",
    "# . v_opt - the estimate covariance\n",
    "#   v_ij_opt - the maximizing cross correlation matrix at (i,j)\n",
    "def PSOF(N, M, joint_covariance, unknown_index_matrix, step_func=func_constant_step_size, max_iteration=12000):\n",
    "    assert (N, N, M, M) == joint_covariance.shape\n",
    "    assert (N, N) == unknown_index_matrix.shape\n",
    "    V = joint_covariance\n",
    "    B = np.zeros((N, M, M), dtype=np.float32)\n",
    "    Lambda_inv_sqrt = np.zeros((N,M), dtype=np.float32)\n",
    "    U = np.zeros((N, M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        Lambda_inv_sqrt[i], U[i] = np.linalg.eigh(joint_covariance[i, i])\n",
    "        Lambda_inv_sqrt[i] = np.reciprocal(np.sqrt(Lambda_inv_sqrt[i]))\n",
    "        B[i] = np.matmul(np.diag(Lambda_inv_sqrt[i]), np.transpose(U[i]))\n",
    "    \n",
    "    V_prime = np.zeros((N, N, M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j and (not unknown_index_matrix[i, j]):\n",
    "                V_prime[i, j] = np.matmul(np.diag(Lambda_inv_sqrt[i]), np.transpose(U[i]))\n",
    "                V_prime[i, j] = np.matmul(V_prime[i, j], V[i, j])\n",
    "                V_prime[i, j] = np.matmul(V_prime[i, j], U[j])\n",
    "                V_prime[i, j] = np.matmul(V_prime[i, j], np.diag(Lambda_inv_sqrt[j]))\n",
    "    \n",
    "    Sigma = np.zeros((M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        Sigma = Sigma + np.linalg.inv(V[i, i])\n",
    "    Sigma_inv = np.linalg.inv(Sigma)\n",
    "\n",
    "    # initial A_prime values\n",
    "    A_prime = np.zeros((N, M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        A_prime[i] = np.matmul(U[i], np.diag(np.reciprocal(Lambda_inv_sqrt[i]))) / N\n",
    "\n",
    "    epislon = 1.0e-12\n",
    "    mse = np.finfo(np.float32).max\n",
    "    mse_iteration = []\n",
    "    mse_best = np.finfo(np.float32).max\n",
    "    last_mse_diff = 0\n",
    "    A_prime_best = np.zeros((N, M, M), dtype=np.float32)\n",
    "    for iteration in range(max_iteration):\n",
    "\n",
    "        # Get SVD of A[j]^T A[i]\n",
    "        C = np.zeros((N, N, M, M), dtype=np.float32)\n",
    "        D = np.zeros((N, N, M, M), dtype=np.float32)\n",
    "        Lambda_ij = np.zeros((N, N, M), dtype=np.float32)\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j and unknown_index_matrix[i, j]:\n",
    "                    C[i, j], Lambda_ij[i, j], D[i, j] = np.linalg.svd(np.matmul(np.transpose(A_prime[j]), A_prime[i]))\n",
    "                    D[i, j] = np.transpose(D[i, j]) # convention of python linalg library\n",
    "                    \n",
    "        # compute subgradients\n",
    "        dA_prime = np.zeros((N, M, M), np.float32)\n",
    "        for i in range(N):\n",
    "            dA_prime[i] = A_prime[i]\n",
    "            for j in range(N):\n",
    "                if j != i:\n",
    "                    if unknown_index_matrix[i, j]:\n",
    "                        dA_prime[i] = dA_prime[i] + np.matmul(np.matmul(A_prime[j], C[i, j]), np.transpose(D[i, j]))\n",
    "                    else:\n",
    "                        dA_prime[i] = dA_prime[i] + np.matmul(A_prime[j], np.transpose(V_prime[i, j]))\n",
    "            dA_prime[i] = dA_prime[i] * 2.0\n",
    "\n",
    "        # apply step size & subgradient\n",
    "        step = step_func(dA_prime, iteration, last_mse_diff)\n",
    "        for i in range(N):\n",
    "            A_prime[i] = A_prime[i] - dA_prime[i] * step\n",
    "            \n",
    "        # project onto the constraint hyperplanes\n",
    "        A_prime_dot_B = np.zeros((M, M), np.float32)\n",
    "        for i in range(N):\n",
    "            A_prime_dot_B = A_prime_dot_B + np.matmul(A_prime[i], B[i])\n",
    "        A_prime_dot_B_Sigma_inv = np.matmul(np.eye(M, dtype=np.float32) - A_prime_dot_B, Sigma_inv)\n",
    "        for i in range(N):\n",
    "            A_prime[i] = A_prime[i] + np.matmul(A_prime_dot_B_Sigma_inv, np.transpose(B[i]))\n",
    "\n",
    "        # compute mse\n",
    "        mse_prime = 0.0\n",
    "        for i in range(N):\n",
    "            mse_prime = mse_prime + np.trace(np.matmul(np.transpose(A_prime[i]), A_prime[i]))\n",
    "        for i in range(N):\n",
    "            for j in range(N):\n",
    "                if i != j:\n",
    "                    if unknown_index_matrix[i, j]:\n",
    "                        _, sigmas, _ = np.linalg.svd(np.matmul(np.transpose(A_prime[j]), A_prime[i]))\n",
    "                        mse_prime = mse_prime + np.sum(sigmas)\n",
    "                    else:\n",
    "                        mse_prime = mse_prime + np.trace(np.matmul(np.matmul(A_prime[i], V_prime[i, j]), np.transpose(A_prime[j])))\n",
    "        \n",
    "        mse_iteration.append(mse_prime)\n",
    "        #print('mse: ', mse_prime, mse_best, sigmas, A_prime)\n",
    "        if mse_prime < mse_best:\n",
    "            mse_best = mse_prime\n",
    "            A_prime_best = A_prime\n",
    "            last_mse_diff = 0\n",
    "        last_mse_diff = mse_prime - mse_best\n",
    "    \n",
    "    A = np.zeros((N, M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        A[i] = np.matmul(np.matmul(A_prime_best[i], np.diag(Lambda_inv_sqrt[i])), np.transpose(U[i]))\n",
    "    V_ij_opt = np.zeros((N, N, M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j and unknown_index_matrix[i, j]:\n",
    "                C_ij, Lambda_ij, D_ij = np.linalg.svd(np.matmul(np.transpose(A_prime_best[j]), A_prime_best[i]))\n",
    "                part_left = np.matmul(U[i], np.diag(np.reciprocal(Lambda_inv_sqrt[i])))\n",
    "                part_middle = np.matmul(np.transpose(D_ij), np.transpose(C_ij))\n",
    "                part_right = np.matmul(np.diag(np.reciprocal(Lambda_inv_sqrt[j])), np.transpose(U[j]))\n",
    "                V_ij_opt[i, j] = np.matmul(np.matmul(part_left, part_middle), part_right)\n",
    "    V_opt = np.zeros((M, M), dtype=np.float32)\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "            if i != j and unknown_index_matrix[i, j]:\n",
    "                V_opt = V_opt + np.matmul(np.matmul(A[i], V_ij_opt[i, j]), np.transpose(A[j]))\n",
    "            else:\n",
    "                V_opt = V_opt + np.matmul(np.matmul(A[i], V[i, j]), np.transpose(A[j]))\n",
    "    \n",
    "    return A, mse_best, V_opt, V_ij_opt, mse_iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distributed filtering with PSOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_index_matrix = np.array([[False, True, True], [True, False, True], [True, True, False]], dtype=bool)\n",
    "N = 3\n",
    "M = 3\n",
    "\n",
    "two_layers_psof_feedback_variances = []\n",
    "x_3 = x_2 = x_1 = x_0\n",
    "p_3 = p_2 = p_1 = p_0\n",
    "y_1_tplus1 = y_1_t = np.zeros((1,1))\n",
    "y_2_tplus1 = y_2_t = np.zeros((1,1))\n",
    "y_3_tplus1 = y_3_t = np.zeros((1,1))\n",
    "two_layers_psof_feedback_variances.append(p_1)\n",
    "for _ in range(iteration_count):\n",
    "    x_1, p_1, _ = local_kalman(x_1, p_1, y_1_tplus1, y_1_t, \\\n",
    "                            phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                            q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t)\n",
    "    x_2, p_2, _ = local_kalman(x_2, p_2, y_2_tplus1, y_2_t, \\\n",
    "                            phi_t, gamma_t, h_2_tplus1, h_2_t, \\\n",
    "                            q_t, r_2_tplus1, r_2_t, s_2_t, b_2_t, u_2_t)\n",
    "    x_3, p_3, _ = local_kalman(x_3, p_3, y_3_tplus1, y_3_t, \\\n",
    "                            phi_t, gamma_t, h_3_tplus1, h_3_t, \\\n",
    "                            q_t, r_3_tplus1, r_3_t, s_3_t, b_3_t, u_3_t)\n",
    "    # Second stage data fusion with PSOF\n",
    "    joint_covariance = np.array([[p_1, p_1, p_1],[p_2, p_2, p_2], [p_3, p_3, p_3]], np.float32)\n",
    "    _, _, p_0_n, _, _ = PSOF(N, M, joint_covariance, unknown_index_matrix)\n",
    "    p_1 = p_2 = p_3 = p_0_n\n",
    "    two_layers_psof_feedback_variances.append(p_0_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monte Carlo simulation for distributed filtering based on PSOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, pytz\n",
    "simulation_rounds = 50\n",
    "iteration_count = 150\n",
    "\n",
    "w_t = np.random.normal(0.0, math.sqrt(sigma_w_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_1 = np.random.normal(0.0, math.sqrt(sigma_zeta_1_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_2 = np.random.normal(0.0, math.sqrt(sigma_zeta_2_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "zeta_3 = np.random.normal(0.0, math.sqrt(sigma_zeta_3_square), \\\n",
    "                       (iteration_count, simulation_rounds))\n",
    "measure_noise_1 = alpha_1 * w_t + zeta_1\n",
    "measure_noise_2 = alpha_2 * w_t + zeta_2\n",
    "measure_noise_3 = alpha_3 * w_t + zeta_3\n",
    "\n",
    "x_o = np.zeros((3, iteration_count+1, simulation_rounds), dtype=float)\n",
    "\n",
    "for r in range(simulation_rounds):\n",
    "    \n",
    "    print(f\"simulatin round {r}, \", datetime.datetime.now().astimezone(pytz.timezone(\"America/Los_Angeles\")))\n",
    "    y_3_tplus1 = y_2_tplus1 = y_1_tplus1 = 0.0\n",
    "    p_3 = p_2 = p_1 = p_0\n",
    "    init_std = math.sqrt(p_0[0, 0])\n",
    "    # Start with correlated initialization\n",
    "    x_3 = x_2 = x_1 = np.random.normal(0.0, init_std, (3,1))\n",
    "    sampled_node = x_1\n",
    "    x_o[0, 0, r] = sampled_node[0,0]\n",
    "    x_o[1, 0, r] = sampled_node[1,0]\n",
    "    x_o[2, 0, r] = sampled_node[2,0]\n",
    "    for i in range(iteration_count):\n",
    "        y_1_t = y_1_tplus1\n",
    "        y_2_t = y_2_tplus1\n",
    "        y_3_t = y_3_tplus1\n",
    "        # Measurement noises are still independent\n",
    "        y_1_tplus1 = measure_noise_1[i, r]\n",
    "        y_2_tplus1 = measure_noise_2[i, r]\n",
    "        y_3_tplus1 = measure_noise_3[i, r]\n",
    "        x_1, p_1, _ = local_kalman(x_1, p_1, np.full((1,1), y_1_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_1_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_1_tplus1, h_1_t, \\\n",
    "                                q_t, r_1_tplus1, r_1_t, s_1_t, b_1_t, u_1_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        x_2, p_2, _ = local_kalman(x_2, p_2, np.full((1,1), y_2_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_2_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_2_tplus1, h_2_t, \\\n",
    "                                q_t, r_2_tplus1, r_2_t, s_2_t, b_2_t, u_2_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        x_3, p_3, _ = local_kalman(x_3, p_3, np.full((1,1), y_3_tplus1, dtype=float), \\\n",
    "                                np.full((1,1), y_3_t, dtype=float), \\\n",
    "                                phi_t, gamma_t, h_3_tplus1, h_3_t, \\\n",
    "                                q_t, r_3_tplus1, r_3_t, s_3_t, b_3_t, u_3_t, \\\n",
    "                                w_t=np.full((1,1), w_t[i,r], dtype=float))\n",
    "        \n",
    "        # Data fusion assuming independence\n",
    "        joint_covariance = np.array([[p_1, p_1, p_1],[p_2, p_2, p_2], [p_3, p_3, p_3]], np.float32)\n",
    "        A, _, p_0_n, _, _ = PSOF(N, M, joint_covariance, unknown_index_matrix)\n",
    "        x_0_n = A[0] @ x_1 + A[1] @ x_2 + A[2] @ x_3\n",
    "        x_3 = x_2 = x_1 = x_0_n\n",
    "        p_3 = p_2 = p_1 = p_0_n\n",
    "        #print(p_1, p_2, p_3, p_0_n)\n",
    "        #print(A)\n",
    "        \n",
    "        # Track (and plot) data fusion in the first node\n",
    "        sampled_node = x_1\n",
    "        x_o[0, i+1, r] = sampled_node[:1]\n",
    "        x_o[1, i+1, r] = sampled_node[1:2]\n",
    "        x_o[2, i+1, r] = sampled_node[2:]\n",
    "x_o_psof_vars = np.var(x_o, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plottings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reproduce graphs in Sun's paper for sanity checking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(16, 4))\n",
    "\n",
    "xlims = [(0, 100), (0, 100), (0, 10)]\n",
    "ylims = [(0.05, 0.15), (0.07, 0.2), (1, 4)]\n",
    "labels = ['position', 'velocity', 'acceleration']\n",
    "for i in range(3):\n",
    "    axes[i].set_ylim(ylims[i])\n",
    "    axes[i].set_xlim(xlims[i])\n",
    "    axes[i].set_xlabel('time step')\n",
    "    axes[i].set_ylabel(f'{labels[i]} variance')\n",
    "    #axes[i].plot(x_o_vars[i,:], label=\"Distibuted crude\")\n",
    "    axes[i].plot([p[0][i,i] for p in local_kf_variances], label='local filter 1')\n",
    "    axes[i].plot([p[1][i,i] for p in local_kf_variances], label='local filter 2')\n",
    "    axes[i].plot([p[2][i,i] for p in local_kf_variances], label='local filter 3')\n",
    "    #axes[i].plot([p[i,i] for p in two_layers_psof_feedback_variances], label='Distributed PSOF')\n",
    "    axes[i].plot([p[i,i] for p in two_layers_kf_variances], label='2-layer filtering')\n",
    "    axes[i].plot([p[i,i] for p in centralized_kf_variances], label='centralized KF')\n",
    "    axes[i].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**My own plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "xlims = (0, 150)\n",
    "ylims = (0.0, 0.4)\n",
    "plt.ylim(ylims)\n",
    "plt.xlim(xlims)\n",
    "plt.xlabel('Simulation time step')\n",
    "plt.ylabel(f'position variance')\n",
    "plt.plot(x_o_vars[0,:], label=\"Distibuted crude\")\n",
    "plt.plot(x_o_psof_vars[0,:], label=\"PSOF Monte Carlo\")\n",
    "plt.plot([p[0,0] for p in two_layers_psof_feedback_variances], label='Dist. PSOF')\n",
    "plt.plot([p[0,0] for p in centralized_kf_variances], label='centralized KF')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
